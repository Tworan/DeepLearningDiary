{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_label), (test_data, test_label) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = dict(\n",
    "    [(value, key) for (key, value) in word_index.items()]\n",
    "    )\n",
    "decoded_review = ' '.join(\n",
    "    [reverse_word_index.get(i-3, '?') for i in train_data[0]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1\n",
    "    return results\n",
    "\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(\n",
    "        layers.Dense(16, activation='relu', input_shape=(10000,), kernel_regularizer=regularizers.l2(0.001))\n",
    "    )\n",
    "    model.add(\n",
    "        layers.Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.001))\n",
    "    )\n",
    "    model.add(\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(\n",
    "        layers.Dense(16, activation='relu', input_shape=(10000,))\n",
    "    )\n",
    "    model.add(\n",
    "        layers.Dense(16, activation='relu')\n",
    "    )\n",
    "    model.add(\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.K_split import K_data_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Process---0\n",
      "Epoch 1/100\n",
      "33/33 [==============================] - 3s 69ms/step - loss: 0.6940 - accuracy: 0.5032 - val_loss: 0.6921 - val_accuracy: 0.5146\n",
      "Epoch 2/100\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.6919 - accuracy: 0.5259 - val_loss: 0.6907 - val_accuracy: 0.5298\n",
      "Epoch 3/100\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.6902 - accuracy: 0.5511 - val_loss: 0.6896 - val_accuracy: 0.5421\n",
      "Epoch 4/100\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.6890 - accuracy: 0.5618 - val_loss: 0.6884 - val_accuracy: 0.5502\n",
      "Epoch 5/100\n",
      "33/33 [==============================] - 1s 27ms/step - loss: 0.6874 - accuracy: 0.5765 - val_loss: 0.6870 - val_accuracy: 0.5639\n",
      "Epoch 6/100\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.6855 - accuracy: 0.5897 - val_loss: 0.6852 - val_accuracy: 0.5751\n",
      "Epoch 7/100\n",
      "33/33 [==============================] - 1s 27ms/step - loss: 0.6838 - accuracy: 0.5983 - val_loss: 0.6829 - val_accuracy: 0.5944\n",
      "Epoch 8/100\n",
      "33/33 [==============================] - 1s 28ms/step - loss: 0.6813 - accuracy: 0.6103 - val_loss: 0.6799 - val_accuracy: 0.6139\n",
      "Epoch 9/100\n",
      "33/33 [==============================] - 1s 28ms/step - loss: 0.6786 - accuracy: 0.6251 - val_loss: 0.6760 - val_accuracy: 0.6295\n",
      "Epoch 10/100\n",
      "33/33 [==============================] - 1s 27ms/step - loss: 0.6746 - accuracy: 0.6411 - val_loss: 0.6712 - val_accuracy: 0.6462\n",
      "Epoch 11/100\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.6708 - accuracy: 0.6516 - val_loss: 0.6658 - val_accuracy: 0.6570\n",
      "Epoch 12/100\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.6645 - accuracy: 0.6674 - val_loss: 0.6598 - val_accuracy: 0.6744\n",
      "Epoch 13/100\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.6578 - accuracy: 0.6829 - val_loss: 0.6535 - val_accuracy: 0.6895\n",
      "Epoch 14/100\n",
      "33/33 [==============================] - 1s 28ms/step - loss: 0.6494 - accuracy: 0.7010 - val_loss: 0.6468 - val_accuracy: 0.7031\n",
      "Epoch 15/100\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.6444 - accuracy: 0.7065 - val_loss: 0.6400 - val_accuracy: 0.7170\n",
      "Epoch 16/100\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.6368 - accuracy: 0.7182 - val_loss: 0.6334 - val_accuracy: 0.7325\n",
      "Epoch 17/100\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.6307 - accuracy: 0.7262 - val_loss: 0.6266 - val_accuracy: 0.7398\n",
      "Epoch 18/100\n",
      "33/33 [==============================] - 1s 21ms/step - loss: 0.6242 - accuracy: 0.7373 - val_loss: 0.6197 - val_accuracy: 0.7426\n",
      "Epoch 19/100\n",
      "33/33 [==============================] - 1s 24ms/step - loss: 0.6152 - accuracy: 0.7420 - val_loss: 0.6132 - val_accuracy: 0.7550\n",
      "Epoch 20/100\n",
      "33/33 [==============================] - 1s 28ms/step - loss: 0.6099 - accuracy: 0.7538 - val_loss: 0.6066 - val_accuracy: 0.7634\n",
      "Epoch 21/100\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.6025 - accuracy: 0.7601 - val_loss: 0.6001 - val_accuracy: 0.7677\n",
      "Epoch 22/100\n",
      "33/33 [==============================] - 1s 27ms/step - loss: 0.5975 - accuracy: 0.7632 - val_loss: 0.5938 - val_accuracy: 0.7718\n",
      "Epoch 23/100\n",
      "33/33 [==============================] - 1s 25ms/step - loss: 0.5884 - accuracy: 0.7735 - val_loss: 0.5875 - val_accuracy: 0.7742\n",
      "Epoch 24/100\n",
      "33/33 [==============================] - 1s 25ms/step - loss: 0.5821 - accuracy: 0.7763 - val_loss: 0.5815 - val_accuracy: 0.7776\n",
      "Epoch 25/100\n",
      "33/33 [==============================] - 1s 22ms/step - loss: 0.5746 - accuracy: 0.7839 - val_loss: 0.5755 - val_accuracy: 0.7830\n",
      "Epoch 26/100\n",
      " 3/33 [=>............................] - ETA: 0s - loss: 0.5657 - accuracy: 0.8062"
     ]
    }
   ],
   "source": [
    "k = 3\n",
    "val_num_samples = len(x_train)\n",
    "for i in range(k):\n",
    "    log_dir = '/home/oneran/Mycodes/DeepLearning/log/imdb'+str(datetime.datetime.now())\n",
    "    tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    print('Process---'+str(i))\n",
    "    train_data, train_labels, val_data, val_labels = K_data_split(k, x_train, train_label, i)\n",
    "    model = build_model()\n",
    "    model.compile(\n",
    "        optimizer='adagrad',\n",
    "        metrics=['accuracy'],\n",
    "        loss='binary_crossentropy'\n",
    "    )\n",
    "    history = model.fit(\n",
    "        train_data, train_labels,\n",
    "        epochs=100,\n",
    "        batch_size=512,\n",
    "        validation_data=(val_data, val_labels),\n",
    "        callbacks=tensorboard_callback\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "782/782 [==============================] - 1s 2ms/step - loss: 0.3758 - accuracy: 0.8470\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.37584277987480164, 0.847000002861023]"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "model.evaluate(x_test, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_8\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_21 (Dense)             (None, 512)               5120512   \n_________________________________________________________________\ndense_22 (Dense)             (None, 512)               262656    \n_________________________________________________________________\ndense_23 (Dense)             (None, 1)                 513       \n=================================================================\nTotal params: 5,383,681\nTrainable params: 5,383,681\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}