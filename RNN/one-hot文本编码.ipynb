{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = ['The cat sat on the table.', 'My dog ate my homework.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The\ncat\nsat\non\nthe\ntable.\nMy\ndog\nate\nmy\nhomework.\n"
     ]
    }
   ],
   "source": [
    "token_index = {}                #单词级的one-hot编码\n",
    "for sample in samples:\n",
    "    for words in sample.split():\n",
    "        if words not in token_index.keys():\n",
    "            token_index[words] = len(token_index) + 1\n",
    "\n",
    "max_length = 10\n",
    "\n",
    "results = np.zeros(\n",
    "    shape=(len(sample), max_length, max(token_index.values()) + 1)      #sampel 文本序列条数    max_length  前max_length个词汇     第三个为axim\n",
    ")\n",
    "\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, word in enumerate(sample.split()[:max_length]):\n",
    "        print(word)\n",
    "        index = token_index.get(word)\n",
    "        results[i, j, index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(10, 12)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "results[0].shape"
   ]
  },
  {
   "source": [
    "字符级的编码"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "characters = string.printable\n",
    "token_index = dict(zip(characters, range(1, len(characters) + 1)))\n",
    "\n",
    "max_length = 50\n",
    "results = np.zeros((len(sample), max_length, max(token_index.values()) + 1))\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, character in enumerate(sample[:max_length]):\n",
    "        index = token_index.get(character)\n",
    "        results[i, j, index] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(23, 50, 101)"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "results.shape"
   ]
  },
  {
   "source": [
    "用keras实现单词级别的one-hot编码"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "samples = ['The cat sat on the table.', 'My dog ate my homework.']\n",
    "\n",
    "tokenizer = Tokenizer(num_words=20)           #考虑最常见的1000个词汇\n",
    "tokenizer.fit_on_texts(samples)     #构建单词索引\n",
    "sequences = tokenizer.texts_to_sequences(samples)       #单词列表\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')    #one-hot编码 向量形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "one_hot_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'num_words': 20,\n",
       " 'filters': '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
       " 'lower': True,\n",
       " 'split': ' ',\n",
       " 'char_level': False,\n",
       " 'oov_token': None,\n",
       " 'document_count': 2,\n",
       " 'word_counts': '{\"the\": 2, \"cat\": 1, \"sat\": 1, \"on\": 1, \"table\": 1, \"my\": 2, \"dog\": 1, \"ate\": 1, \"homework\": 1}',\n",
       " 'word_docs': '{\"sat\": 1, \"cat\": 1, \"the\": 1, \"on\": 1, \"table\": 1, \"my\": 1, \"ate\": 1, \"dog\": 1, \"homework\": 1}',\n",
       " 'index_docs': '{\"4\": 1, \"3\": 1, \"1\": 1, \"5\": 1, \"6\": 1, \"2\": 1, \"8\": 1, \"7\": 1, \"9\": 1}',\n",
       " 'index_word': '{\"1\": \"the\", \"2\": \"my\", \"3\": \"cat\", \"4\": \"sat\", \"5\": \"on\", \"6\": \"table\", \"7\": \"dog\", \"8\": \"ate\", \"9\": \"homework\"}',\n",
       " 'word_index': '{\"the\": 1, \"my\": 2, \"cat\": 3, \"sat\": 4, \"on\": 5, \"table\": 6, \"dog\": 7, \"ate\": 8, \"homework\": 9}'}"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "tokenizer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{1: 'the',\n",
       " 2: 'my',\n",
       " 3: 'cat',\n",
       " 4: 'sat',\n",
       " 5: 'on',\n",
       " 6: 'table',\n",
       " 7: 'dog',\n",
       " 8: 'ate',\n",
       " 9: 'homework'}"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[1, 3, 4, 5, 1, 6], [2, 7, 8, 2, 9]]"
      ]
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "source": [
    "one_hot_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}